{"paragraphs":[{"text":"import org.apache.spark._\nimport org.apache.spark.rdd._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.mllib.feature.HashingTF\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport scala.util.{Success, Try}\n    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n    var chatDF = hiveContext.sql(\"select chat from [database].[table] where date like '%2013-09%'\")\n    chatDF.count()\n    chatDF.show()","dateUpdated":"2019-01-10T21:05:23-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612886_-631218534","id":"20170314-222922_910635069","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9129"},{"text":"var messages = chatDF.select(\"chat\")\nprintln(\"Total messages: \" + messages.count())\n\nvar happyMessages = messages.filter(messages(\"chat\").contains(\"awesome\"))\nval countHappy = happyMessages.count()\nprintln(\"Number of happy messages: \" +  countHappy)\n\nvar unhappyMessages = messages.filter(messages(\"chat\").contains(\"suck\"))\nval countUnhappy = unhappyMessages.count()\nprintln(\"Unhappy Messages: \" + countUnhappy)\n\nval smallest = Math.min(countHappy, countUnhappy).toInt\n\n//Create a dataset with equal parts happy and unhappy messages\nvar chats = happyMessages.limit(smallest).unionAll(unhappyMessages.limit(smallest))\n    ","dateUpdated":"2019-01-10T20:45:59-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612891_-633142279","id":"20170314-222925_680225414","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9130"},{"text":"%md\n### Label the Data and force our model to learn the difference between happy and sad.","user":"nm563754","dateUpdated":"2019-01-10T20:49:23-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612895_-634681274","id":"20170315-215800_466665578","dateCreated":"2019-01-10T20:43:32-0600","dateStarted":"2019-01-10T20:49:23-0600","dateFinished":"2019-01-10T20:49:23-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9131","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Label the Data and force our model to learn the difference between happy and sad.</h3>\n"}]}},{"text":"val messagesRDD = chats.rdd\n//We use scala's Try to filter out tweets that couldn't be parsed\nval goodBadRecords = messagesRDD.map(\n  row =>{\n    Try{\n      val msg = row(0).toString.toLowerCase()\n      var isHappy:Int = 0\n      if(msg.contains(\"suck\")){\n        isHappy = 0\n      }else if(msg.contains(\"awesome\")){\n        isHappy = 1\n      }\n      var msgSanitized = msg.replaceAll(\"awesome\", \"\")\n      msgSanitized = msgSanitized.replaceAll(\"suck\",\"\")\n      //Return a tuple\n      (isHappy, msgSanitized.split(\" \").toSeq)\n    }\n  }\n)\n\n//We use this syntax to filter out exceptions\nval exceptions = goodBadRecords.filter(_.isFailure)\nprintln(\"total records with exceptions: \" + exceptions.count())\nexceptions.take(10).foreach(x => println(x.failed))\nvar labeledTweets = goodBadRecords.filter((_.isSuccess)).map(_.get)\nprintln(\"total records with successes: \" + labeledTweets.count())\n","dateUpdated":"2019-01-10T20:46:36-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612898_-648147486","id":"20170314-234521_264925171","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9132"},{"text":"%md\nLet's take a look at our progress.","user":"nm563754","dateUpdated":"2019-01-10T20:49:30-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612900_-650455979","id":"20170315-220802_1466536053","dateCreated":"2019-01-10T20:43:32-0600","dateStarted":"2019-01-10T20:49:30-0600","dateFinished":"2019-01-10T20:49:30-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9133","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's take a look at our progress.</p>\n"}]}},{"text":"labeledTweets.take(10).foreach(x => println(x))\n","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612903_-650071230","id":"20170315-221309_2084520780","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9134"},{"text":"%md\n### Transform Data\nGradient Boosting expects as input a vector (feature array) of fixed length, so we need a way to convert our chats into some numeric vector that represents that chat.","dateUpdated":"2019-01-10T20:49:36-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612904_-651994975","id":"20170315-222108_1329422295","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9135","user":"nm563754","dateFinished":"2019-01-10T20:49:36-0600","dateStarted":"2019-01-10T20:49:36-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Transform Data</h3>\n<p>Gradient Boosting expects as input a vector (feature array) of fixed length, so we need a way to convert our chats into some numeric vector that represents that chat.</p>\n"}]}},{"text":"    val hashingTF = new HashingTF(2000)\n\n    //Map the input strings to a tuple of labeled point + input text\n    val input_labeled = (labeledTweets.map(\n      t => (t._1, hashingTF.transform(t._2)))\n      .map(x => new LabeledPoint((x._1).toDouble, x._2)))","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612906_-651225477","id":"20170315-221527_265576053","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9136"},{"text":"%md\nLet's take a look at how our vectors are hashed.","dateUpdated":"2019-01-10T20:49:41-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612907_-651610226","id":"20170315-225630_2078720791","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9137","user":"nm563754","dateFinished":"2019-01-10T20:49:41-0600","dateStarted":"2019-01-10T20:49:41-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's take a look at how our vectors are hashed.</p>\n"}]}},{"text":"input_labeled.count()\ninput_labeled.take(5).foreach(println)","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612908_-653533971","id":"20170315-225826_221402586","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9138"},{"text":"%md\nAs you can see, we've converted each chat into a vector of integers. This will work great for a machine learning model, but we want to preserve some chats in a form we can read.","dateUpdated":"2019-01-10T20:49:46-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612910_-652764473","id":"20170315-225921_1191286770","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9139","user":"nm563754","dateFinished":"2019-01-10T20:49:46-0600","dateStarted":"2019-01-10T20:49:46-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>As you can see, we've converted each chat into a vector of integers. This will work great for a machine learning model, but we want to preserve some chats in a form we can read.</p>\n"}]}},{"text":"//We're keeping the raw text for inspection later\nvar sample = (labeledTweets.take(1000).map(\n  t => (t._1, hashingTF.transform(t._2), t._2))\n .map(x => (new LabeledPoint((x._1).toDouble, x._2), x._3)))","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612911_-653149222","id":"20170315-225625_1635512137","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9140"},{"text":"%md\n### Split into Training and Validation Sets\n#### Fixing overfitting: \n\nIf you see that your validation accuracy is very low compared to your training accuracy, you can fix this overfitting by either increasing the size of your training data or by decreasing the number of parameters in your model.","dateUpdated":"2019-01-10T20:49:51-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612912_-642761001","id":"20170315-230331_1819827530","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9141","user":"nm563754","dateFinished":"2019-01-10T20:49:51-0600","dateStarted":"2019-01-10T20:49:51-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Split into Training and Validation Sets</h3>\n<h4>Fixing overfitting:</h4>\n<p>If you see that your validation accuracy is very low compared to your training accuracy, you can fix this overfitting by either increasing the size of your training data or by decreasing the number of parameters in your model.</p>\n"}]}},{"text":"\n    // Split the data into training and validation sets (30% held out for validation testing)\n    val splits = input_labeled.randomSplit(Array(0.3, 0.7))\n    val (trainingData, validationData) = (splits(0), splits(1))","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612914_-641991504","id":"20170315-220757_1010718454","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9142"},{"text":"%md\n### Build the Model\n\nThis machine learning application uses Gradient Boosting for classification.","dateUpdated":"2019-01-10T20:49:57-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612915_-642376252","id":"20170315-233003_2105288198","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9143","user":"nm563754","dateFinished":"2019-01-10T20:49:57-0600","dateStarted":"2019-01-10T20:49:57-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Build the Model</h3>\n<p>This machine learning application uses Gradient Boosting for classification.</p>\n"}]}},{"text":"\n    val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n    boostingStrategy.setNumIterations(20) //number of passes over our training data\n    boostingStrategy.treeStrategy.setNumClasses(2) //We have two output classes: happy and sad\n    boostingStrategy.treeStrategy.setMaxDepth(6) \n    //Depth of each tree. Higher numbers mean more parameters, which can cause overfitting.\n    //Lower numbers create a simpler model, which can be more accurate. \n    //In practice you have to tweak this number to find the best value.\n\n    val model = GradientBoostedTrees.train(trainingData, boostingStrategy)","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612916_-644299997","id":"20170315-232951_1402100499","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9144"},{"text":"%md\n### Evaluate Model","dateUpdated":"2019-01-10T20:50:02-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612918_-643530499","id":"20170315-234102_792111976","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9145","user":"nm563754","dateFinished":"2019-01-10T20:50:02-0600","dateStarted":"2019-01-10T20:50:02-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Evaluate Model</h3>\n"}]}},{"text":"// Evaluate model on test instances and compute test error\nvar labelAndPredsTrain = trainingData.map { point =>\n  val prediction = model.predict(point.features)\n  Tuple2(point.label, prediction)\n}\n\nvar labelAndPredsValid = validationData.map { point =>\n  val prediction = model.predict(point.features)\n  Tuple2(point.label, prediction)\n}\n\n//Since Spark has done the heavy lifting already, lets pull the results back to the driver machine.\n//Calling collect() will bring the results to a single machine (the driver) and will convert it to a Scala array.\n\n//Start with the Training Set\nval results = labelAndPredsTrain.collect()\n\nvar happyTotal = 0\nvar unhappyTotal = 0\nvar happyCorrect = 0\nvar unhappyCorrect = 0\nresults.foreach(\n  r => {\n    if (r._1 == 1) {\n      happyTotal += 1\n    } else if (r._1 == 0) {\n      unhappyTotal += 1\n    }\n    if (r._1 == 1 && r._2 ==1) {\n      happyCorrect += 1\n    } else if (r._1 == 0 && r._2 == 0) {\n      unhappyCorrect += 1\n    }\n  }\n)\nprintln(\"unhappy messages in Training Set: \" + unhappyTotal + \" happy messages: \" + happyTotal)\nprintln(\"happy % correct: \" + happyCorrect.toDouble/happyTotal)\nprintln(\"unhappy % correct: \" + unhappyCorrect.toDouble/unhappyTotal)\n\nval testErr = labelAndPredsTrain.filter(r => r._1 != r._2).count.toDouble / trainingData.count()\nprintln(\"Test Error Training Set: \" + testErr)\n\n\n//Compute error for validation Set\nval results = labelAndPredsValid.collect()\n\nvar happyTotal = 0\nvar unhappyTotal = 0\nvar happyCorrect = 0\nvar unhappyCorrect = 0\nresults.foreach(\n  r => {\n    if (r._1 == 1) {\n      happyTotal += 1\n    } else if (r._1 == 0) {\n      unhappyTotal += 1\n    }\n    if (r._1 == 1 && r._2 ==1) {\n      happyCorrect += 1\n    } else if (r._1 == 0 && r._2 == 0) {\n      unhappyCorrect += 1\n    }\n  }\n)\nprintln(\"unhappy messages in Validation Set: \" + unhappyTotal + \" happy messages: \" + happyTotal)\nprintln(\"happy % correct: \" + happyCorrect.toDouble/happyTotal)\nprintln(\"unhappy % correct: \" + unhappyCorrect.toDouble/unhappyTotal)\n\nval testErr = labelAndPredsValid.filter(r => r._1 != r._2).count.toDouble / validationData.count()\nprintln(\"Test Error Validation Set: \" + testErr)\n","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612919_-643915248","id":"20170315-233951_1248887406","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9146"},{"text":"%md\n### Disecting the data at the individual chat level.","dateUpdated":"2019-01-10T20:50:10-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612920_-645838993","id":"20180727-201640_1086570864","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9147","user":"nm563754","dateFinished":"2019-01-10T20:50:10-0600","dateStarted":"2019-01-10T20:50:10-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Disecting the data at the individual chat level.</h3>\n"}]}},{"text":"//Print some examples and how they scored\nval predictions = sample.map { point =>\n  val prediction = model.predict(point._1.features)\n  (point._1.label, prediction, point._2)\n}\n\n//The first entry is the true label. 1 is happy, 0 is unhappy. \n//The second entry is the prediction.\npredictions.take(100).foreach(x => println(\"label: \" + x._1 + \" prediction: \" + x._2 + \" text: \" + x._3.mkString(\" \")))","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612922_-645069495","id":"20170315-235355_1294226754","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9148"},{"text":"%md\nOnce you've trained your first model, you should go back and tweak the model parameters to see if you can increase model accuracy. In this case, try tweaking the depth of each tree and the number of iterations over the training data. You could also let the model see a greater percentage of happy tweets than unhappy tweets to see if that improves prediction accuracy for happy tweets.\n","dateUpdated":"2019-01-10T20:50:18-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612923_-645454244","id":"20170316-002506_341852523","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9149","user":"nm563754","dateFinished":"2019-01-10T20:50:18-0600","dateStarted":"2019-01-10T20:50:18-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Once you've trained your first model, you should go back and tweak the model parameters to see if you can increase model accuracy. In this case, try tweaking the depth of each tree and the number of iterations over the training data. You could also let the model see a greater percentage of happy tweets than unhappy tweets to see if that improves prediction accuracy for happy tweets.</p>\n"}]}},{"text":"%md\n### Exporting the Model\n\nOnce your model is as accurate as you can make it, you can export it for production use. Models trained with Spark can be easily loaded back into a Spark Streaming workflow for use in production.","dateUpdated":"2019-01-10T20:50:22-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612925_-647762737","id":"20170315-235739_1251499727","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9150","user":"nm563754","dateFinished":"2019-01-10T20:50:22-0600","dateStarted":"2019-01-10T20:50:22-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Exporting the Model</h3>\n<p>Once your model is as accurate as you can make it, you can export it for production use. Models trained with Spark can be easily loaded back into a Spark Streaming workflow for use in production.</p>\n"}]}},{"text":"//model.save(sc, \"hdfs:///tmp/tweets/RandomForestModel\")","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612926_-646608490","id":"20170315-235824_1541028619","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9151"},{"text":"%md\nYou've now seen how to build a sentiment analysis model. The techniques you've seen here can be applied to other text classification models besides sentiment analysis. Try analyzing other keywords besides happy and sad and see what results you get. ","dateUpdated":"2019-01-10T20:50:30-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612927_-646993239","id":"20170315-235646_797080385","dateCreated":"2019-01-10T20:43:32-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9152","user":"nm563754","dateFinished":"2019-01-10T20:50:30-0600","dateStarted":"2019-01-10T20:50:30-0600","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You've now seen how to build a sentiment analysis model. The techniques you've seen here can be applied to other text classification models besides sentiment analysis. Try analyzing other keywords besides happy and sad and see what results you get.</p>\n"}]}},{"text":"//println(model.predict(hashingTF.transform(\"that is awesome\".split(\" \").toSeq)))\nmodel.predict(hashingTF.transform(chatDF.split(\" \").toSeq))","dateUpdated":"2019-01-10T20:43:32-0600","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612929_-661613697","id":"20170316-000532_1106625181","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9153"},{"text":"%sql\n","dateUpdated":"2019-01-10T20:43:32-0600","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1547174612936_-664306940","id":"20181126-200400_374964169","dateCreated":"2019-01-10T20:43:32-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9154"}],"name":"daisy_sentiment_scala","id":"2E1ZM23DZ","angularObjects":{"2CHS8UYQQ:shared_process":[],"2DH56H4NA:shared_process":[],"2CKX6DGQZ:shared_process":[],"2CK8A9MEG:shared_process":[],"2CKX8WPU1:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}